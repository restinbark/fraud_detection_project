# ğŸ›¡ï¸ Fraud Detection Project

A supervised machine learning project designed to detect fraudulent activity using multiple datasets including transactional data, IP geolocation, and anonymized credit card usage records. This project follows a structured pipeline from data cleaning to model interpretability using SHAP.

---

## ğŸ“ Project Structure

fraud_detection_project/
â”‚
â”œâ”€â”€ data/ # Raw datasets
â”œâ”€â”€ outputs/
â”‚ â””â”€â”€ figures/ # Saved plots (EDA, models, SHAP)
â”‚ â””â”€â”€ data/ # Preprocessed CSVs (used in training)
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ task_1_eda.ipynb # EDA + preprocessing
â”‚ â”œâ”€â”€ task_2_modeling.ipynb # Model training and evaluation
â”‚ â”œâ”€â”€ task_3_shap.ipynb # Interpretability
â”œâ”€â”€ models/ # Trained model files
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md # Project documentation

yaml


---

## âœ… Objectives

- Analyze transactional behavior to detect fraud
- Train and evaluate models (Logistic Regression, Random Forest, XGBoost)
- Use SHAP to interpret model decisions
- Recommend the best model for deployment

---

## ğŸ” Task 1: Data Profiling, Cleaning & EDA

### ğŸ§¹ Data Cleaning
- Merged transactional data with IP address geolocation.
- Handled missing values and converted data types.
- Engineered time-based features (e.g., `signup_hour`, `time_diff`).

### ğŸ“Š Univariate & Bivariate Plots

| Plot Title                    | Preview |
|------------------------------|---------|
| Age Distribution             | ![Age Distribution](outputs/figures/age_distribution.png) |
| Purchase Value vs Age        | ![Purchase Value vs Age](outputs/figures/purchase_vs_age.png) |
| Time Difference Distribution | ![Time Difference](outputs/figures/time_diff_distribution.png) |

---

## ğŸ¤– Task 2: Model Training & Evaluation

### ğŸ§  Models Trained
- Logistic Regression
- Random Forest
- XGBoost

### ğŸ“ˆ Model Evaluation Metrics

| Model              | Accuracy | Precision | Recall | F1 Score | ROC AUC |
|-------------------|----------|-----------|--------|----------|---------|
| LogisticRegression| 0.73     | 0.19      | 0.59   | 0.29     | 0.70    |
| Random Forest      | 0.95     | 0.82      | 0.53   | 0.65     | 0.77    |
| XGBoost            | 0.95     | 0.91      | 0.53   | 0.67     | 0.76    |

### ğŸ§¾ Model Visualizations

#### Logistic Regression
- ![Logistic Confusion Matrix](outputs/figures/logistic_confusion_matrix.png)
- ![Logistic ROC](outputs/figures/logistic_roc_curve.png)

#### Random Forest
- ![RF Confusion Matrix](outputs/figures/rf_confusion_matrix.png)
- ![RF ROC](outputs/figures/rf_roc_curve.png)

#### XGBoost
- ![XGBoost Confusion Matrix](outputs/figures/xgb_confusion_matrix.png)
- ![XGBoost ROC](outputs/figures/xgb_roc_curve.png)

---

## ğŸŒ Task 3: Model Interpretability with SHAP

- Applied SHAP to interpret XGBoost predictions.
- Identified most influential features driving fraud classification.

### ğŸ“Š SHAP Visualizations

| Plot Type        | Description               | Preview |
|------------------|---------------------------|---------|
| Global Summary   | Top features globally     | ![SHAP Bar](outputs/figures/shap_global_bar_plot.png) |
| Beeswarm         | Feature value influence   | ![Beeswarm](outputs/figures/shap_beeswarm_plot.png) |
| Waterfall        | Single prediction explainer| ![Waterfall](outputs/figures/shap_waterfall_plot.png) |

---

## ğŸ“Œ Final Recommendation

ğŸ“¦ **Recommended Model: XGBoost**

- Best precision among all
- Balanced tradeoff between recall and accuracy
- Highly interpretable via SHAP

---

## ğŸ“š Requirements

Install all required libraries:

```bash
pip install -r requirements.txt

ğŸ‘¨â€ğŸ’» Author
Barkilign Mulatu | @restinbark
